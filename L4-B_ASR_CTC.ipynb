{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO8FHO1kWtxB2ZsV9gWOtxq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rGSm0DNMhxie"},"source":["# üéôÔ∏è L4-B: Speech Recognition - Connectionist Temporal Classification\n","\n","---\n","\n","Having explored speech features already, let's proceed with end-to-end speech recognition:\n","- L4-A) Speech Features üåä\n","- **L4-B) Speech Recognition - Connectionist Temporal Classification** üï∞Ô∏è\n","\n","Connectionist Temporal Classification (CTC), is an algorithm that addresses the issue of mapping a length variable input sequence to a fixed transcription.\n","\n","For instance, if you try to transcribe someone saying \"hello\", by simply classifying the phonemes at every small audio chunk, you may get \"hhhheeellooo\" or \"heeeellllooo\", for instance. CTC smartly solves this alignment issue, and we'll illustrate in this colab how this is achieved, step by step.\n","\n","We will learn about CTC, by using one of the current state-of-the-art speech recognition models: wav2vec. This consists of a Transformer-based encoder that has been trained with a self-supervised learning method, followed by a classifier that outputs characters.\n","\n","We will be using some of the most trending Python libraries right now: HuggingFace for loading the ASR models, and PyTorch-Lightning for handling the data.\n","\n","The lab will be carried out through the following steps:\n","\n","1) Download and explore LibriSpeech test data.\n","\n","2) Download HuggingFace wav2vec model.\n","\n","3) Transcribe test data with wav2vec. Check the effect of CTC in word error rate (WER) metrics.\n","\n","4) Repeat the previous step, but applying different levels of noise to audio, studying how performance varies depending to noise.\n","\n","5) Try models with our own voice, also with crosslingual models."]},{"cell_type":"markdown","metadata":{"id":"PfOcoOY2jAH5"},"source":["## üì¶ Installs and imports\n","\n","We will use pip once again to install some required libraries for this colab. Key libraries here are:\n","- **pytorch-lightning** - this is a wrapper of PyTorch library, which we are going to use to run inference over our LibriSpeech dataset\n","- **torchaudio** - PyTorch's own audio backend\n","- **transformers** - this is HuggingFace's library, which contains code and weights for wav2vec model"]},{"cell_type":"code","source":["# A few installs for audio recording and processing\n","!sudo apt-get install -q -y timidity libsndfile1\n","!pip install pydub numba jiwer music21 librosa pytorch-lightning torchaudio\n","!pip install -q transformers"],"metadata":{"id":"aJoLkO0lR8Hb"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Esw7gRt50mgY"},"source":["# Generic imports\n","import IPython\n","import os\n","from tqdm import tqdm\n","\n","# Math and Deep Learning imports\n","import numpy as np\n","import pytorch_lightning as pl\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import torchaudio\n","\n","# Audio imports\n","import librosa\n","import librosa.display\n","from base64 import b64decode\n","from IPython.display import Audio, Javascript\n","from pydub import AudioSegment\n","from scipy.io import wavfile\n","\n","# HuggingFace imports\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n","\n","# Word Error Rate import\n","from jiwer import wer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CalmFDKrjD3A"},"source":["## üíæ Dataset loading with PyTorch-Lightning\n","\n","This time, we're going to explore LibriSpeech, an audiobook dataset. It is ideal to train speech recognition models, because it contains many hours of speech (the readers reading) and their corresponding transcription (the book itself).\n","\n","When we train or run inference with neural networks, it is convenient to send more than one input to our network forward pass, to parallelize computation and increase efficiency.\n","\n","For instance, if you want to transcribe 4 audios, you can run 4 forward passes to your network separately... or, you can group these 4 audios in the same tensor, and run a single forward pass. This is what we call **batching** - creating a single **batch** with **N** different inputs.\n","\n","PyTorch-Lightning is a nice wrapper for PyTorch, which is very useful to keep your code functional, tight and clean. We use the following piece of code to add some logic that allows to create such batches. It is not crucial for now to understand every line of code here below, keep in mind the high level idea of batching."]},{"cell_type":"code","metadata":{"id":"tc7JxI2SmDHl"},"source":["class LibrispeechDataModule(pl.LightningDataModule):\n","    def __init__(self, data_dir: str = './data', test_batch_size: int = 2):\n","        super().__init__()\n","\n","        self.data_dir = data_dir\n","        self.test_batch_size = test_batch_size\n","        self.max_size = 60 # maximum sample size is 60 seconds\n","        self.sample_rate = 16000\n","\n","    def prepare_data(self):\n","        torchaudio.datasets.LIBRISPEECH(self.data_dir, url='test-clean', download=True)\n","\n","    def setup(self, stage=None):\n","        self.test_dataset = torchaudio.datasets.LIBRISPEECH(self.data_dir, url='test-clean', download=False)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.test_batch_size, collate_fn=self.collater, shuffle=False)\n","\n","    def collater(self, samples):\n","        waveforms = []\n","        labels = []\n","        for (waveform, _, utterance, _, _, _) in samples:\n","            if waveform.size(1) > self.max_size*self.sample_rate:\n","                pass\n","            else:\n","                waveforms.append(waveform.squeeze())\n","                labels.append(utterance)\n","\n","        waveforms = nn.utils.rnn.pad_sequence(waveforms, batch_first=True).unsqueeze(1)\n","\n","        return waveforms, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zdtkJuam6Gd"},"source":["# Run this cell so our LibriSpeech data module is initialized, and the dataset is downloaded.\n","if not os.path.isdir(\"./data\"):\n","    os.makedirs(\"./data\")\n","\n","dm = LibrispeechDataModule(data_dir='./data')\n","dm.prepare_data()\n","dm.setup()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6OP9agiXov8r"},"source":["## üéß Audio visualization"]},{"cell_type":"code","source":["# Run this to extract a batch from the test set, containing waveforms and their corresponding transcripts.\n","for batch in dm.test_dataloader():\n","    waveforms, transcripts = batch\n","    break"],"metadata":{"id":"fVt3YGSDVxrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjdMIxHznHgC"},"source":["# Let's now inspect one of these waveforms: listening to it and checking the transcription.\n","index = 0\n","\n","print(f\"Waveform shape = {waveforms[index].shape}\")\n","waveform = waveforms[index]\n","transcript = transcripts[index]\n","\n","print(f\"Transcript = {transcript}\")\n","librosa.display.waveshow(waveform.squeeze().numpy(), sr=16000)\n","IPython.display.Audio(waveform.squeeze().numpy(), rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have a waveform and the ground truth transcription, let's use wav2vec model to transcribe it automatically, and exemplify how CTC works.\n","\n","As we have the ground truth transcription, we can check how the predicted transcription scores with the Word Error Rate (WER) measurement."],"metadata":{"id":"tSPudfCPWLho"}},{"cell_type":"markdown","metadata":{"id":"fJ6OldkLo6Xy"},"source":["## ü§ñ Model\n","\n","Let's download the wav2vec2.0 model from the HuggingFace repo. We mainly need two things:\n","- A **tokenizer** - which is used to handle everything related to characters. It transforms input strings into numerical token sequences, and also converts output probabilities from the main model into readable characters.\n","- The **model** - this has all the wav2vec logic required to extract features and compute output probabilities for character sequences."]},{"cell_type":"code","metadata":{"id":"CGEQ066L8uBu"},"source":["# First, we will need a tokenizer to translate the model output probabilities to readable characters.\n","tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","\n","# Then, download the wav2vec model itself, which will output character probabilities from raw waveforms.\n","model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5khr4igxrlsg"},"source":["## ‚úçÔ∏è Transcribing a waveform\n","\n","___\n","\n","Now, we'll transcribe a waveform, with the downloaded model and tokenizer. You'll be implementing some of the calls, so in case of doubt, you can check the documentation here: https://huggingface.co/transformers/v4.3.3/model_doc/wav2vec2.html#wav2vec2forctc"]},{"cell_type":"markdown","source":["‚úçÔ∏è **TASK 1-A** (+0.50/10) - Now, pass the LibriSpeech waveform seen before, through the wav2vec model. How does the output look like?"],"metadata":{"id":"3RJZYxMsYERa"}},{"cell_type":"code","metadata":{"id":"y4P-hMWppVwG"},"source":["output = None # <- implement the forward pass for wav2vec here.\n","print(output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚úçÔ∏è **TASK 1-B** (+0.50/10) - Extract the logits and take a look at them, specially at their shape."],"metadata":{"id":"k22FEjOjYSEr"}},{"cell_type":"code","metadata":{"id":"KMNRF8F4qnnC"},"source":["output_logits = None # <- extract output logits here.\n","print(output_logits)\n","print(output_logits.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYSIzgqhrCsY"},"source":["# The shape should be (B, T, C).\n","# B = batch size (1 for now, since we're only passing one waveform)\n","# T = time steps (the number of output frames in the time dimension)\n","# C = output class number (the number of possible output tokens, the letters in the English alphabet, plus some other characters like ' or the whitespace.\n","tokenizer.get_vocab()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚úçÔ∏è **TASK 1-C** (+0.75/10) - OK, now that you have the output logits, translate them into characters."],"metadata":{"id":"dOZvHfXbYg2F"}},{"cell_type":"code","metadata":{"id":"MpSNlxestVaf"},"source":["# First, for each time step T, get the index of the biggest logit, aka the one with the highest probability.\n","# Clue: use torch.argmax(...), and remember to torch.squeeze the output.\n","predicted_ids = torch.argmax(...)\n","\n","# Second, pass the IDs through the tokenizer in order to convert them into tokens.\n","# Clue: use the convert_ids_to_tokens(...) function, in the tokenizer.\n","predicted_tokens = None\n","print(predicted_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1J2G4dkPuehP"},"source":["# Well, that didn't look that nice, right? Let's clean up the padding tokens, and replace | tokens with whitespaces.\n","predicted_string = ''\n","for token in predicted_tokens:\n","    if token == '<pad>':\n","        pass\n","    elif token == '|':\n","        predicted_string += ' '\n","    else:\n","        predicted_string += token\n","\n","predicted_string = ' '.join(predicted_string.split())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eysUK6riwCno"},"source":["# Let's compare the predicted string, with the ground truth reference one.\n","print(\"Predicted transcript:\")\n","print(predicted_string)\n","print(\"Reference transcript:\")\n","print(transcript)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚úçÔ∏è **TASK 1-D** (+0.50/10) - Almost there, right? However, can we give a metric about the error rate in the predicted transcription, regarding the reference?"],"metadata":{"id":"F4K2gZjAYwTf"}},{"cell_type":"code","metadata":{"id":"xy7LSfZwxJrz"},"source":["# Use the jiwer library, in order to compute the word error rate (WER) score. It has been already installed and imported at the beginning.\n","# See: https://pypi.org/project/jiwer/\n","\n","word_error_rate = None\n","print(f\"WER = {word_error_rate*100.0}%\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nbJplZZyWA6"},"source":["# Obviously, the WER is quite high since we have many repeated characters.\n","# Phonemes may span across several time frames, leading to repeated letters.\n","# CTC algorithm solves this. The idea is to remove consecutive duplicated characters. \"CAAAATT\" -> \"CAT\"\n","# Wait! What if our target word has repeated characters, like \"DINNER\"?\n","# That's why CTC algorithm introduces the \"pad\"/\"blank\" token. This is a \"dummy\" token that will be removed,\n","# and causes that the network learns to predict it between consecutive letters that must be together.\n","\n","# Correct output\n","# \"DIIIIN<pad>NEEER\" -> \"DINNER\"\n","\n","# Incorrect output\n","# \"DIINNNNER\" -> \"DINER\"\n","\n","# There is a function in tokenizer that automatically applies the CTC algorithm to the output tokens, yielding the correct string transcription.\n","# Can you find it? Look at the source code at: https://huggingface.co/transformers/v4.3.3/_modules/transformers/models/wav2vec2/tokenization_wav2vec2.html#Wav2Vec2Tokenizer\n","# Or call the following function to see the tokenizer function members. One clue, the function name starts with \"convert_tokens_...\"\n","dir(tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚úçÔ∏è **TASK 1-E** (+0.75/10) - Convert the tokens to a string with CTC algorithm."],"metadata":{"id":"K_V7BeyFZCSG"}},{"cell_type":"code","metadata":{"id":"4JJ8rWOs1ENz"},"source":["predicted_string = None # <- implement the correct function for CTC algorithm here\n","\n","# Now, compute again the WER, and check if it has improved or not.\n","print(\"Predicted transcript:\")\n","print(predicted_string)\n","print(\"Reference transcript:\")\n","print(transcript)\n","\n","word_error_rate = None # <- compute the WER once again here\n","print(f\"WER = {word_error_rate*100.0}%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnFYRDlsURwZ"},"source":["## ü•û Transcribing batches\n","____\n","\n","‚úçÔ∏è **TASK 2** (+2.00/10) - As before, we'll be transcribing waveforms, but this time in batches of two waveforms at the same time.\n","You'll be implementing the calls to the waveform forward through the model, plus the decoding of the output logits into a text string.\n","\n","You can implement what you learned before. This time, you can directly translate the predicted IDs into text strings with tokenizer's function \"batch_decode(...)\".\n","\n","‚ö†Ô∏è - The dataset contains more than 1000 batches, so transcribing everything would take too much time. We will only compute metrics over the first 10 batches, so don't worry, the process should stop automatically after 10 forward passes.\n","\n"]},{"cell_type":"code","metadata":{"id":"mofdsyuJ2XO8"},"source":["# Let's compute the WER of 10 batches of 2 samples from the LibriSpeech test set.\n","preds = []\n","refs = []\n","stop_at = 10\n","iteration = 0\n","for batch in tqdm(dm.test_dataloader()):\n","    waveforms, transcripts = batch\n","    refs += transcripts\n","\n","    print(waveforms.shape)\n","    # TASK 2: obtain the transcript from the waveform.\n","    # TASK 2a: go from waveforms -> logits\n","    output_logits = None\n","    # TASK 2b: go from logits -> predicted token IDs, with torch.argmax\n","    predicted_ids = None\n","    # TASK 2c: go from predicted token IDs, to the final text string, with tokenizer's batch_decode(...) function.\n","    pred_transcripts = None\n","\n","    preds += pred_transcripts\n","\n","    iteration += 1\n","\n","    if iteration == stop_at:\n","        break\n","\n","test_wer = wer(refs, preds)\n","print(f\"\\nLibri test WER = {test_wer*100.0}%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚úçÔ∏è **TASK 3** (+2.00/10) - For now, we've operated with audio that is quite clean, but what if there are noises or severe distortions?\n","\n","I want you to artificially add noise to the same audios we've transcribed in batches before. Then, you'll be transcribing the noised audios, and you'll measure how much WER is deteriorated.\n","\n","The idea is that you repeat the same inference loop as in **TASK 2**, but increasing the noise everytime. You shall then plot the relationship between the level of noise and WER.\n","\n","We will control the level of noise by the **signal-to-noise ratio (SNR)**. A high SNR (like 50) means that there is more speech (signal) than noise. A low SNR (like -10) means that there is way more noise than speech (signal).\n","\n","Step by step, you'll have to:\n","\n","- 1) Implement again the same inference loop as in **TASK 2**.\n","- 2) But, add noise to the waveforms. You have an utility function for that here below.\n","- 3) Get the average WER for the 10-sample test set several times. Every time, use a different SNR to control how much noise you put there.\n","- 4) Plot the relationship of SNR (x-axis) vs WER (y-axis).\n","- 5) Comment and analyze what you see.\n","\n"],"metadata":{"id":"orK1fNuPMjf8"}},{"cell_type":"code","source":["# Utility function to add noise to a waveform.\n","def add_noise_to_waveform(waveform, snr_db):\n","    \"\"\"\n","    Adds Gaussian noise to a waveform at a given SNR (in dB) using PyTorch.\n","\n","    Args:\n","        waveform (torch.Tensor): Input waveform (1D PyTorch tensor).\n","        snr_db (float): Desired Signal-to-Noise Ratio in decibels (dB).\n","\n","    Returns:\n","        torch.Tensor: Waveform with added noise at the specified SNR.\n","    \"\"\"\n","\n","    # Calculate the signal power (mean of squared waveform values)\n","    signal_power = torch.mean(waveform**2)\n","\n","    # Convert SNR from dB to a linear scale\n","    snr_linear = 10**(snr_db / 10)\n","\n","    # Calculate the noise power\n","    noise_power = signal_power / snr_linear\n","\n","    # Generate Gaussian noise with the computed noise power\n","    noise = torch.sqrt(noise_power) * torch.randn(waveform.shape)\n","\n","    # Add the noise to the original waveform\n","    noisy_waveform = waveform + noise\n","\n","    return noisy_waveform"],"metadata":{"id":"Y5D6MNWgKYOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_snrs = [] # Define here the values of SNRs that you want to test.\n","y_wers = [] # You'll be storing here the average WER value of the test set, as a\n","            # function of the SNR used to apply noise to the test waveforms.\n","\n","# Reimplement the loop from TASK 2 here, but this time using different SNR\n","# values and collecting their corresponding WERs, in order to plot afterwards\n","# their relation.\n"],"metadata":{"id":"Lg_LXyMqEWHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot here the relation between SNRs (x-axis) and WERs (y-axis)\n"],"metadata":{"id":"bw-Wl7OyQJml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reason here what you see about the relation between SNR and WER.\n","print(\"\")"],"metadata":{"id":"nSY6XmJKQPAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yolzGAsdG66w"},"source":["## ü§© Try it with your own voice\n","---\n","\n","If you want to have some fun, you can run the following cells to record and transcribe yourself (microphone required). You'll be able to try in English, and also other languages."]},{"cell_type":"code","metadata":{"id":"8lnbN4mt5C-2"},"source":["#@title [Run this] Definition of the JS code to record audio straight from the browser\n","\n","RECORD = \"\"\"\n","const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n","const b2text = blob => new Promise(resolve => {\n","  const reader = new FileReader()\n","  reader.onloadend = e => resolve(e.srcElement.result)\n","  reader.readAsDataURL(blob)\n","})\n","var record = time => new Promise(async resolve => {\n","  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n","  recorder = new MediaRecorder(stream)\n","  chunks = []\n","  recorder.ondataavailable = e => chunks.push(e.data)\n","  recorder.start()\n","  await sleep(time)\n","  recorder.onstop = async ()=>{\n","    blob = new Blob(chunks)\n","    text = await b2text(blob)\n","    resolve(text)\n","  }\n","  recorder.stop()\n","})\n","\"\"\"\n","\n","def record(sec=5, out_name='recorded_audio.wav'):\n","  try:\n","    from google.colab import output\n","  except ImportError:\n","    print('No possible to import output from google.colab')\n","    return ''\n","  else:\n","    print('Recording')\n","    display(Javascript(RECORD))\n","    s = output.eval_js('record(%d)' % (sec*1000))\n","    fname = out_name\n","    print('Saving to', fname)\n","    b = b64decode(s.split(',')[1])\n","    with open(fname, 'wb') as f:\n","      f.write(b)\n","    return fname"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cINoe6oBJo7Q"},"source":["EXPECTED_SAMPLE_RATE = 16000\n","\n","def convert_audio_for_model(user_file, output_file='converted_audio_file.wav'):\n","  audio = AudioSegment.from_file(user_file)\n","  audio = audio.set_frame_rate(EXPECTED_SAMPLE_RATE).set_channels(1)\n","  audio.export(output_file, format=\"wav\")\n","  return output_file\n","\n","def record_utterance(file_name='my_recording_wav'):\n","    file_name = record(5, file_name)\n","    converted_audio_file = convert_audio_for_model(file_name)\n","    input_audio, sr = torchaudio.load(converted_audio_file)\n","    return input_audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlK5er2uWaCm"},"source":["Run the cell below to record yourself, the recording will be automatically stopped at 5 seconds, approx. Say whatever you want, in English!"]},{"cell_type":"code","metadata":{"id":"Dn67bovNLHjt"},"source":["input_audio = record_utterance('english_recording.wav')\n","print(input_audio.shape)\n","librosa.display.waveshow(input_audio.squeeze().numpy(), sr=16000)\n","Audio(input_audio.squeeze().numpy(), rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pm-IGUrRUfiM"},"source":["def transcribe_audio(input_audio, asr_model, asr_tokenizer):\n","    output_logits = asr_model(input_audio).logits\n","    predicted_ids = torch.argmax(output_logits, dim=-1)\n","    transcription = asr_tokenizer.batch_decode(predicted_ids)\n","    return transcription"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time"],"metadata":{"id":"6UAVyTnOIYkH"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qj0hlLA6HmVo"},"source":["#start_time = time.time()\n","transcribe_audio(input_audio, model, tokenizer)\n","#print(f\"Elapsed time = {time.time() - start_time} s\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFRkY3LpVGT1"},"source":["## üåê Changing languages\n","\n","___\n","\n","There are further wav2vec models at HuggingFace that are able to transcribe in different languages. Maybe you want to try other language rather than English. Look for the crosslingual models, those containing the \"xlsr\" keyword, like \"facebook/wav2vec2-large-xlsr-53-arabic\"\n","\n","Models are found here: https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"]},{"cell_type":"code","metadata":{"id":"_err3PAMVFh8"},"source":["# Load a tokenizer and a model from the XLSR models, in the language you want to try.\n","#xlsr_tokenizer =\n","#xlsr_model =\n","# Solution\n","xlsr_tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"ccoreilly/wav2vec2-large-xlsr-catala\")\n","xlsr_model = Wav2Vec2ForCTC.from_pretrained(\"ccoreilly/wav2vec2-large-xlsr-catala\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOja0C3VWrki"},"source":["Now, record yourself talking in your chosen language!"]},{"cell_type":"code","metadata":{"id":"W0BEfhNYWpQF"},"source":["input_audio = record_utterance('xlsr_recording.wav')\n","print(input_audio.shape)\n","librosa.display.waveshow(input_audio.squeeze().numpy(), sr=16000)\n","Audio(input_audio.squeeze().numpy(), rate=16000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-CilyFP_fyF"},"source":["# Transcription with the English model\n","print(transcribe_audio(input_audio, model, tokenizer))\n","\n","# Transcription with the Cross-lingual model\n","print(transcribe_audio(input_audio, xlsr_model, xlsr_tokenizer))"],"execution_count":null,"outputs":[]}]}