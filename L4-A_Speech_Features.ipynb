{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1DTV0IWdkeEXAMJPsmYZUYN1oTZZJssUO","timestamp":1621768295202}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"XLj8jNJlLTFI"},"source":["# üéôÔ∏è L4-A: Speech Features\n","\n","---\n","\n","Welcome to the first lab (L4) about speech recognition! L4 is divided in 2 parts, each dedicated to different topics:\n","- **L4-A) Speech Features** üåä\n","- L4-B) Speech Recognition - Connectionist Temporal Classification üï∞Ô∏è\n","\n","In this colab, L4-A, we will learn the basics about features used for speech tasks. This will be the first stepping stone before we dive deep into speech recognition (and synthesis, but that will be another day).\n","\n","Get ready to learn how to load audio waveforms into tensors, and how to compute the most relevant features for speech processing, the spectrograms!\n"]},{"cell_type":"markdown","source":["## üì¶ Imports\n","\n","First, we will use **pip** to install **torchaudio**, a library built on top of **PyTorch**, that allows to load audio directly into **torch** tensors, allowing for fast and efficient usage in neural networks and Graphical Processing Units (GPU)! üî•"],"metadata":{"id":"IBXG5xHrmFrQ"}},{"cell_type":"code","metadata":{"id":"O8LavTOhNROD"},"source":["!pip install torchaudio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's import some libraries that we will use for math and tensor operations (**numpy**, **torch**), as well as **torchaudio** and some plotting utilities (**librosa.display**, **matplotlib**, **IPython**)."],"metadata":{"id":"QnufF6QHm-T4"}},{"cell_type":"code","source":["# Math & tensor libs\n","import numpy as np\n","import torch\n","\n","# Audio backend\n","import torchaudio\n","\n","# Plotting libs\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import IPython"],"metadata":{"id":"sfPpk4i3m8iV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üíæ Dataset\n","\n","We just need to download some speech audio files. In this case, we will use audio from the Google Speech Commands dataset, which contains recordings of simple voice commands like \"stop\", \"go\", \"yes\", \"no\", etc."],"metadata":{"id":"4ZONKrORnyMp"}},{"cell_type":"code","source":["# Torchaudio provides a function to directly download this data. Download could take 1-2 minutes.\n","# You can visit the documentation for more information - https://pytorch.org/audio/stable/generated/torchaudio.datasets.SPEECHCOMMANDS.html#torchaudio.datasets.SPEECHCOMMANDS\n","dataset = torchaudio.datasets.SPEECHCOMMANDS(\n","    root='./',\n","    url='speech_commands_v0.01',\n","    download=True,\n","    subset='testing',\n",")"],"metadata":{"id":"dwm03cXroK_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This dataset contains many files, how do we get one?\n","# We can use __getitem__ function, which takes the number/id of the sample we want to retrieve\n","sample_id = 0\n","sample = dataset.__getitem__(sample_id)"],"metadata":{"id":"WLaq7dwdpnet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample"],"metadata":{"id":"r_s4MiuUuncB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That's not really easy to understand, right? You can take a look at the documentation to understand what is returned there, [click on this link](https://pytorch.org/audio/stable/generated/torchaudio.datasets.SPEECHCOMMANDS.html#torchaudio.datasets.SPEECHCOMMANDS).\n","\n","Basically, we have:\n","- I) a tensor containing the audio waveform\n","- II) the sampling rate\n","- III) the label, what is being said in the audio\n","- IV) the identity of the speaker\n","- V) the identifier of the utterance\n","\n","Let's familiarize ourselves with each concept!"],"metadata":{"id":"B7KKuwAUt5qt"}},{"cell_type":"markdown","source":["## üéß Audio visualization"],"metadata":{"id":"gDK1hy2d0_5R"}},{"cell_type":"code","source":["audio, sampling_rate, label, speaker_id, utterance_id = sample"],"metadata":{"id":"MtZQvJdPpzjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Starting with the audio, we have it stored on a torch tensor. We can visualize a few characteristics of it.\n","audio_shape = audio.shape\n","print(f\"Shape of the audio tensor = {audio_shape}\")\n","print(f\"You see it has two dimensions: The first = {audio_shape[0]} and the second = {audio_shape[1]}\")\n","print(f\"The first corresponds to the number of channels, in this case 1, because it is a mono audio. It would be 2, if it was stereo, as the songs you listen daily!\")\n","print(f\"The second is the temporal length, meaning that our audio is represented by {audio_shape[1]} numbers. Because our sampling rate is 16000 Hz, it means that {audio_shape[1]} / 16000 = 1 second\")\n","print(\"___________________________\\n\")\n","print(\"Let's visualize and listen to the audio! You can see that the audio lasts 1 second indeed.\")\n","print(f\"Also, pay attention to what is being said, it should correspond to the label = {label}\")\n","\n","librosa.display.waveshow(audio.numpy(), sr=sampling_rate)\n","IPython.display.Audio(audio, rate=sampling_rate)"],"metadata":{"id":"qVhGQ-Nqurr4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's play with the sampling rate to get familiarized with such concept.\n","\n","The sampling rate means how many numbers (or samples) you need to codify one second of audio, and it is measured in Hertz = Hz.\n","\n","As the previous sample has sample rate of 16000 Hz, we had to inform that to the visualization library.\n","\n","What if we input that the sampling rate is, for instance, 8000 Hz? Input waveform has 16000 samples, so 16000 / 8000 = 2 seconds.\n","\n","The library will interpret that the audio is two seconds long, which is wrong, and it will sound slowed down..."],"metadata":{"id":"alxN9Q9I0Ntz"}},{"cell_type":"code","source":["IPython.display.Audio(audio, rate=8000)"],"metadata":{"id":"sSG6QKEeyZ5K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Likewise, if we input a higher sampling rate, like 32000 Hz, the audio will sound sped up."],"metadata":{"id":"9sM_DW6s0dIC"}},{"cell_type":"code","source":["IPython.display.Audio(audio, rate=32000)"],"metadata":{"id":"F80LruW10JbF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Lesson** - it is important to know what is the sampling rate of our recorded audio, and adapt our data processing pipeline to match it!"],"metadata":{"id":"y2MveQyP0hiB"}},{"cell_type":"markdown","source":["Feel free to go back and modify `sample_id = 0`, (e.g. `sample_id = 1`), and rerun the previous cells to inspect other samples."],"metadata":{"id":"qjNLvE1OyA32"}},{"cell_type":"markdown","source":["## ‚õèÔ∏è Speech features extraction"],"metadata":{"id":"rJ9BOJYP1KOS"}},{"cell_type":"markdown","source":["Now, let's extract the most commonly used features for speech processing. Summarizing:\n","- I) The waveform contains the most information, but is costly to process. You see, 16000 samples to characterize someone saying \"bed\"! In text, \"bed\" only has 3 characters... 16000 >> 3, so in complexity audio >> text üòØ\n","- II) The spectrogram (or Short-Time Fourier Transform = STFT), means decomposing 1D audio (time) into a 2D matrix of (frequency, time). This feature contains more explicit information and is more compressed, easier to process.\n","- III) The mel-spectrogram is a very popular choice, where we filter the spectrogram to enhance and reduce certain frequencies, according to how the human ear perceives them.\n","- IV) The MFCC (Mel Frequency Cepstral Coefficients) is a further step of compressing even more the mel-spectrogram, making it more efficient to process, especially for shallow neural networks, at the cost of some information lost."],"metadata":{"id":"dYxAwOqV1gbX"}},{"cell_type":"markdown","source":["### üåä The waveform\n","\n","We already inspected the waveform before. Thanks to **torchaudio**, we downloaded a waveform in a tensor format.\n","\n","What if we want to save our waveform tensor to our local disk? You may use **torchaudio.save** for that."],"metadata":{"id":"09GxwtWz4CGa"}},{"cell_type":"code","source":["!ls"],"metadata":{"id":"oAKD8m355CWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["waveform_local_path = \"./waveform_0.wav\"  #: the local path where we save the audio\n","audio_to_save = audio #: the tensor to be saved as a wav file\n","output_sampling_rate = sampling_rate #: the sampling rate at which we save the audio\n","torchaudio.save(uri=waveform_local_path, src=audio_to_save, sample_rate=output_sampling_rate)\n","!ls"],"metadata":{"id":"l0vstDFn4eRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, the audio is saved now in our local disk!\n","Before, we downloaded the audio tensor directly from the Internet, thanks to torchaudio. But what if we want to read a waveform that is stored locally in our disk, and put it into a torch tensor?\n","\n","‚úçÔ∏è **TASK 1** (+0.75/10) - Load the audio waveform that you just stored in the local disk, into a torch tensor. Use **torchaudio.load** function for that, [see the documentation here.](https://pytorch.org/audio/stable/generated/torchaudio.load.html#torchaudio.load)"],"metadata":{"id":"hRGWl2Ss5tO5"}},{"cell_type":"code","source":["def load_audio(path):\n","  # TASK 1: Implement audio waveform loading with Torchaudio here.\n","  waveform, waveform_sampling_rate = None, None\n","  return waveform, waveform_sampling_rate\n","\n","waveform, waveform_sampling_rate = load_audio(waveform_local_path)\n","\n","# You'll have to pass these tests to ensure that you loaded the audio correctly.\n","assert torch.is_tensor(waveform), f\"Error! waveform is not a torch tensor, is: {type(waveform)}.\"\n","assert waveform.shape == (1, 16000), f\"Error! The waveform shape detected is: {waveform.shape}\"\n","assert waveform_sampling_rate == 16000, f\"Error! The waveform sampling rate is not the expected one: {waveform_sampling_rate}\"\n","\n","librosa.display.waveshow(waveform.numpy(), sr=waveform_sampling_rate)\n","IPython.display.Audio(waveform, rate=waveform_sampling_rate)"],"metadata":{"id":"dlZlhdUC6qPr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚úçÔ∏è **TASK 2** (+0.75/10) - Now, let's extract the spectrogram or STFT from the waveform. Use **torchaudio.transforms.Spectrogram** function for that, [see the documentation here.](https://pytorch.org/audio/main/generated/torchaudio.transforms.Spectrogram.html#torchaudio.transforms.Spectrogram)"],"metadata":{"id":"EQotFl578eiH"}},{"cell_type":"code","source":["def extract_spectrogram(wav, n_fft, win_length, hop_length):\n","  # TASK 2: Implement spectrogram extractor here.\n","  spectrogram_extractor = torchaudio.transforms.Spectrogram(...) #: < - implement the correct argument calls here!\n","  spectrogram = spectrogram_extractor(wav)\n","  return spectrogram\n","\n","# Utility function to visualize spectrograms.\n","def plot_spectrogram(spec, title=None, ylabel='Frequency Bins', aspect='auto', xmax=None):\n","  fig, axs = plt.subplots(1, 1)\n","  axs.set_title(title or 'Spectrogram (db)')\n","  axs.set_ylabel(ylabel)\n","  axs.set_xlabel('Temporal Frames')\n","  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n","  if xmax:\n","    axs.set_xlim((0, xmax))\n","  fig.colorbar(im, ax=axs)\n","  plt.show(block=False)\n","\n","# Arguments required for spectrogram computation. Feel free to play with them.\n","N_FFT = 400  # Number of Fast Fourier Transformer (FFT) points. Controls frequency resolution. Higher value gives more detail in frequency, but lower time resolution.\n","WIN_LENGTH = 400  # Window size in samples. Determines the length of each segment analyzed by the FFT.\n","HOP_LENGTH = 200  # Step size between windows in samples. Controls overlap between segments. Smaller values increase overlap, capturing smoother transitions.\n","\n","spectrogram = extract_spectrogram(waveform, n_fft=N_FFT, win_length=WIN_LENGTH, hop_length=HOP_LENGTH)\n","\n","assert len(spectrogram.shape) == 3\n","\n","print(f\"Spectrogram shape is {spectrogram.shape}\")\n","print(f\"Dimension 0 is {spectrogram.shape[0]}, which means there is one spectrogram in this tensor batch.\")\n","print(f\"Dimension 1 is {spectrogram.shape[1]}, which means that there are {spectrogram.shape[1]} frequency bins.\")\n","print(f\"Dimension 2 is {spectrogram.shape[2]}, which means that there are {spectrogram.shape[2]} temporal frames.\")\n","\n","plot_spectrogram(spectrogram[0, :, :], title=\"Spectrogram\")"],"metadata":{"id":"e8jupAPI-q34"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚úçÔ∏è **TASK 3** (+0.75/10) - Now, let's apply the mel scale to convert our spectrogram into a mel-spectrogram. Use **torchaudio.transforms.MelScale** function for that, [see the documentation here.](https://pytorch.org/audio/main/_modules/torchaudio/transforms/_transforms.html#MelScale)"],"metadata":{"id":"Zu6oXAGoB3-u"}},{"cell_type":"code","source":["def extract_mel_spectrogram(wav, n_mels, n_fft, window_length, hop_length):\n","    # TASK 3: Implement mel scale transform here.\n","    to_mel_scale = torchaudio.transforms.MelScale(...) #: < - implement the correct argument calls here!\n","    spectrogram = extract_spectrogram(wav, n_fft, window_length, hop_length)\n","    mel_spectrogram = to_mel_scale(spectrogram)\n","    return mel_spectrogram\n","\n","# Arguments required for mel-spectrogram computation. Feel free to play with them.\n","N_MELS = 40 # Number of Mel bands. Controls the number of frequency bins in the mel-spectrogram. A higher number gives more frequency detail.\n","mel_spectrogram = extract_mel_spectrogram(waveform, n_mels=N_MELS, n_fft=N_FFT, window_length=WIN_LENGTH, hop_length=HOP_LENGTH)\n","\n","assert len(mel_spectrogram.shape) == 3\n","\n","print(f\"Mel-Spectrogram shape is {mel_spectrogram.shape}\")\n","print(f\"Dimension 0 is {mel_spectrogram.shape[0]}, which means there is one mel-spectrogram in this tensor batch.\")\n","print(f\"Dimension 1 is {mel_spectrogram.shape[1]}, which means that there are {mel_spectrogram.shape[1]} mel frequency bins.\")\n","print(f\"Dimension 2 is {mel_spectrogram.shape[2]}, which means that there are {mel_spectrogram.shape[2]} temporal frames.\")\n","\n","plot_spectrogram(mel_spectrogram[0, :, :], title=\"Mel-Spectrogram\")"],"metadata":{"id":"DeyGO5x3B0gV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚úçÔ∏è **TASK 4** (+0.75/10) - Finally, let's explore the MFCC. Use **torchaudio.transforms.MFCC** function for that, [see the documentation here.](https://pytorch.org/audio/main/generated/torchaudio.transforms.MFCC.html#torchaudio.transforms.MFCC)"],"metadata":{"id":"WE8XM9jIDnBi"}},{"cell_type":"code","source":["def extract_mfcc(wav, n_mfcc, mel_kwargs):\n","    # TASK 4: Implement MFCC extractor here.\n","    mfcc_extractor = torchaudio.transforms.MFCC(..., **mel_kwargs) # <- implement the correct arguments in this call\n","    mfcc = mfcc_extractor(wav)\n","    return mfcc\n","\n","N_MFCC = 13\n","MEL_KWARGS = {\"n_mels\": N_MELS, \"n_fft\": N_FFT, \"win_length\": WIN_LENGTH, \"hop_length\": HOP_LENGTH}\n","mfcc = extract_mfcc(waveform, n_mfcc=N_MFCC, mel_kwargs=MEL_KWARGS)\n","\n","assert len(mfcc.shape) == 3\n","\n","print(f\"MFCCshape is {mfcc.shape}\")\n","print(f\"Dimension 0 is {mfcc.shape[0]}, which means there is one MFCC in this tensor batch.\")\n","print(f\"Dimension 1 is {mfcc.shape[1]}, which means that there are {mfcc.shape[1]} MFCC bins.\")\n","print(f\"Dimension 2 is {mfcc.shape[2]}, which means that there are {mfcc.shape[2]} temporal frames.\")\n","\n","plot_spectrogram(mfcc[0, :, :], title=\"MFCC\")"],"metadata":{"id":"GWfHF5ZOD4sA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just to foster discussion and thought, let's take a look at the different audio representations (features) we've extracted. Particularly, let's focus on how many samples (numbers) does each one require to characterise our one second input audio."],"metadata":{"id":"Xx6oM_SNFBtx"}},{"cell_type":"code","source":["print(f\"Raw audio waveform shape = {waveform.shape} | Channels = {waveform.shape[0]} | Temporal frames = {waveform.shape[1]} | Total samples = {waveform.shape[0] * waveform.shape[1]}\")\n","print(f\"Spectrogram shape = {spectrogram.shape} | Channels = {spectrogram.shape[1]} | Temporal frames = {spectrogram.shape[2]} | Total samples = {spectrogram.shape[1] * spectrogram.shape[2]}\")\n","print(f\"Mel-Spectrogram shape = {mel_spectrogram.shape} | Channels = {mel_spectrogram.shape[1]} | Temporal frames = {mel_spectrogram.shape[2]} | Total samples = {mel_spectrogram.shape[1] * mel_spectrogram.shape[2]}\")\n","print(f\"MFCC shape = {mfcc.shape} | Channels = {mfcc.shape[1]} | Temporal frames = {mfcc.shape[2]} | Total samples = {mfcc.shape[1] * mfcc.shape[2]}\")"],"metadata":{"id":"MxyhzHqNFOZO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That's it! We have onboarded on speech audio features. This will be useful from now onwards, where we will learn about speech recognition and synthesis for conversational agents. Keep this lesson in mind if you ever also work on general audio."],"metadata":{"id":"uIV_qs5hFPk0"}}]}